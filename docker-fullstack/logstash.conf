input {
  beats {
    port => 5044
  }
  
  tcp {
    port => 5000
    codec => json_lines
  }
  
  udp {
    port => 5000
    codec => json_lines
  }
  
  # Collect Docker logs via Promtail or Filebeat
  http {
    port => 8080
  }
}

filter {
  # Parse container logs
  if [container] {
    mutate {
      add_field => { "log_source" => "docker" }
    }
  }
  
  # Parse Kafka logs
  if [service] == "kafka" {
    grok {
      match => { "message" => "\[%{TIMESTAMP_ISO8601:timestamp}\] %{LOGLEVEL:log_level} %{GREEDYDATA:log_message}" }
    }
    mutate {
      add_field => { "log_source" => "kafka" }
    }
  }
  
  # Parse Airflow logs
  if [service] =~ "airflow" {
    grok {
      match => { "message" => "\[%{TIMESTAMP_ISO8601:timestamp}\] \{%{DATA:filename}:%{INT:line_number}\} %{LOGLEVEL:log_level} - %{GREEDYDATA:log_message}" }
    }
    mutate {
      add_field => { "log_source" => "airflow" }
    }
  }
  
  # Add timestamp
  date {
    match => [ "timestamp", "ISO8601" ]
  }
  
  # Clean up fields
  mutate {
    remove_field => [ "host", "agent", "ecs", "input" ]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logs-%{+YYYY.MM.dd}"
    template_name => "logstash"
    template_pattern => "logs-*"
    template => {
      "index_patterns" => ["logs-*"]
      "settings" => {
        "number_of_shards" => 1
        "number_of_replicas" => 0
      }
      "mappings" => {
        "properties" => {
          "@timestamp" => { "type" => "date" }
          "message" => { "type" => "text" }
          "log_level" => { "type" => "keyword" }
          "service" => { "type" => "keyword" }
          "container" => { "type" => "keyword" }
          "log_source" => { "type" => "keyword" }
        }
      }
    }
  }
  
  stdout { 
    codec => rubydebug 
  }
}
